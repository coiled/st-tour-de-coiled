{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Python Deployments as a Service\n",
    "\n",
    "### James Bourbeau - Dask Maintainer & Coiled engineer\n",
    "\n",
    "In this talk we'll:\n",
    "\n",
    "1. Give a brief overview of Dask\n",
    "2. Introduce Coiled and what it has to offer\n",
    "3. Spin up a Dask cluster on AWS with Coiled\n",
    "4. Create and use a custom software environment on our cluster\n",
    "5. Answer any questions you have along the way\n",
    "\n",
    "**Goal for this talk: Have an understanding for what Coiled offers and how to get started.**\n",
    "\n",
    "# Dask - tl;dr\n",
    "\n",
    "Dask ([docs](https://docs.dask.org)) is a popular library for parallel and distributed computing in Python. Dask:\n",
    "\n",
    "- Reuses familiar APIs from the PyData ecosystem like NumPy, Pandas, and Scikit-Learn\n",
    "- Integrates with many libraries you may already use: XArray, Rapids, XGBoost, etc.\n",
    "- Works equally well both on a single machine, or running on multiple machines in a cluster\n",
    "  \n",
    "<img src=\"dask-cluster.svg\"\n",
    "     width=\"60%\"\n",
    "     alt=\"Dask cluster\\\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "# Create a Dask cluster on my laptop using local\n",
    "# processes for the scheduler and workers\n",
    "cluster = LocalCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect a client to my local cluster\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"s3://coiled-data/higgs/higgs-00.csv\",\n",
    "    blocksize=\"10 MiB\",\n",
    "    storage_options={\"anon\": True},\n",
    ").persist()\n",
    "\n",
    "df.groupby(\"labels\").missing_energy_magnitude.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great &mdash; we can work on larger-than-memory datasets while using familiar APIs with Dask!\n",
    "\n",
    "The cluster we've been using so far has been running on my laptop. How can I launch a Dask cluster with more computational resources?\n",
    "There are lots of great open source projects for launching clusters on various kinds of hardware:\n",
    "\n",
    "- [Dask-Kubernetes](https://kubernetes.dask.org/en/latest/) for deploying Dask using native Kubernetes APIs\n",
    "- [Dask-Yarn](https://yarn.dask.org/en/latest/) for deploying Dask on YARN clusters\n",
    "- [Dask-Jobqueue](https://jobqueue.dask.org/en/latest/) for deploying Dask on job queuing systems (e.g. PBS, Slurm, etc.)\n",
    "- [Dask Cloud Provider](https://cloudprovider.dask.org/en/latest/) for deploying Dask on cloud-based infrastructure (e.g. AWS Fargate, AzureML)\n",
    "- [Dask-MPI](http://mpi.dask.org/en/latest/) for deploying Dask on existing MPI environments\n",
    "\n",
    "**However**, using these projects (typically) involves manually setting up infrastructure, e.g. a Kubernetes cluster, and having a deep knowledge of the system the cluster is being launched on, e.g. how to create an AWS IAM role with appropriate levels of permissions.\n",
    "\n",
    "Additionally, there are lots of features, like software environment management, that these projects don't address.\n",
    "\n",
    "That's all to say, these projects are great for some people in some situations, but they're not for everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coiled\n",
    "\n",
    "Coiled ([docs](https://docs.coiled.io/)) is a deployment-as-a-service library for scaling Python. Generally, Coiled provides:\n",
    "\n",
    "- Easily launchable, cloud-based Dask clusters\n",
    "- Support for managed software environments\n",
    "- Tools for collaborating and monitoring costs\n",
    "\n",
    "You can try out our public beta at https://cloud.coiled.io.\n",
    "\n",
    "Note that **while Coiled is in beta, it's totally free to use**. You will not be charged for any of the clusters you use, and you can use up to 100 running cores concurrently.\n",
    "\n",
    "To [get started with Coiled](https://docs.coiled.io/user_guide/getting_started.html), install the `coiled` Python package (from conda-forge or PyPI) and then run `coiled login` in your terminal:\n",
    "\n",
    "```bash\n",
    "$ conda install -c conda-forge coiled\n",
    "$ coiled login\n",
    "```\n",
    "\n",
    "# Deploying Dask clusters with Coiled\n",
    "\n",
    "Let's launch our first Dask cluster with Coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(n_workers=10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Note that the scheduler (and workers) are running on AWS and I connect to the remote cluster from my laptop with `client = Client(cluster)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"s3://coiled-data/higgs/higgs-*.csv\",\n",
    "    blocksize=\"10 MiB\",\n",
    "    storage_options={\"anon\": True},\n",
    ").persist()\n",
    "\n",
    "df.groupby(\"labels\").missing_energy_magnitude.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Woo, we just did a groupby-aggregation on a Dask cluster on AWS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first pain point: software environments\n",
    "\n",
    "Let's perform some further analysis with this dataset by using it to train an XGBoost classification model.\n",
    "\n",
    "**Warning**: running the cell below will raise an error and we'll talk about this more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "import dask_xgboost\n",
    "\n",
    "# Extract training features and target label\n",
    "X, y = df.iloc[:, 1:], df[\"labels\"]\n",
    "\n",
    "# Split full dataset into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=2)\n",
    "\n",
    "# Use Dask-XGBoost to train an XGBoost model\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"max_depth\": 3,\n",
    "    \"min_child_weight\": 0.5,\n",
    "}\n",
    "bst = dask_xgboost.train(client, params, X_train, y_train, num_boost_round=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting this `ModuleNotFoundError` because `dask_xgboost` isn't installed on the workers for our cluster, so the workers aren't able to run the Dask-XGBoost tasks we're sending to them.\n",
    "\n",
    "This is one of the first pain points users tend to experience with distributed computing. Now that there is more than one machine involved in our computations, we need to ensure that each machine has the appropriate libraries installed to execute our tasks.\n",
    "\n",
    "# Creating software environments with Coiled\n",
    "\n",
    "Coiled supports [building custom software environments](https://docs.coiled.io/user_guide/software_environment_creation.html) using familiar packaging conventions, like conda and pip, that you're probably already using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "conda = {\n",
    "    \"channels\": [\"conda-forge\"],\n",
    "    \"dependencies\": [\n",
    "        \"python=3.8\",\n",
    "        \"dask\",\n",
    "        \"coiled\",\n",
    "        \"dask-ml\",\n",
    "        \"dask-xgboost\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "coiled.create_software_environment(\n",
    "    name=\"dask-xgboost\",\n",
    "    conda=conda,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    n_workers=10,\n",
    "    software=\"dask-xgboost\",   # The cluster scheduler and workers will use our new \"dask-xgboost\" software environment\n",
    "    worker_cpu=4,\n",
    "    worker_memory=\"8 GiB\",\n",
    ")\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the exact same code again, now that our cluster has `dask_xgboost` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(\n",
    "    \"s3://coiled-data/higgs/higgs-*.csv\",\n",
    "    storage_options={\"anon\": True},\n",
    "    blocksize=\"10 MiB\",\n",
    ").persist()\n",
    "\n",
    "# Extract training features and target label\n",
    "X, y = ddf.iloc[:, 1:], ddf[\"labels\"]\n",
    "\n",
    "# Split full dataset into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=2)\n",
    "\n",
    "# Use Dask-XGBoost to train an XGBoost model\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"max_depth\": 3,\n",
    "    \"min_child_weight\": 0.5,\n",
    "}\n",
    "bst = dask_xgboost.train(client, params, X_train, y_train, num_boost_round=3)\n",
    "bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, it worked! We were able to create a custom software environment fairly straightforwardly using `coiled.create_software_environment` and then spin up a cluster which uses the new software environment.\n",
    "\n",
    "In addition to conda packages, you can also specify pip packages to install or a Docker image to use. For example, we could create a software environment with GPU-accelerated libraries:\n",
    "\n",
    "\n",
    "```python\n",
    "import coiled\n",
    "\n",
    "coiled.create_software_environment(\n",
    "    name=\"gpu-env\",\n",
    "    container=\"gpuci/miniconda-cuda:10.2-runtime-ubuntu18.04\",\n",
    "    conda={\n",
    "        \"channels\": [\"rapidsai\", \"conda-forge\", \"defaults\"],\n",
    "        \"dependencies\": [\"dask\", \"dask-cuda\", \"xgboost\", \"pytorch\"],\n",
    "    },\n",
    ")\n",
    "```\n",
    "\n",
    "See the [Software environments documentation](https://docs.coiled.io/user_guide/software_environment_creation.html) for more complete details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and cost monitoring\n",
    "\n",
    "You can share your software environments and cluster settings with your friends and colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!coiled env inspect dask-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!coiled env inspect necaris/gputest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost tracking\n",
    "\n",
    "Coiled keeps track of resource usage on a per person and per account basis. This let's you monitor exactly how much you're spending.\n",
    "\n",
    "![](clusters-table.png)\n",
    "\n",
    "You can also set usage limits on a per person and per account basis to ensure you don't rack up a large bill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Coiled clusters automatically shut down after 20 minutes of inactivity. This helps prevent large bills when you accidentally leave a cluster running over the weekend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "- Join the Coiled beta at https://cloud.coiled.io\n",
    "- See the [Coiled docs](https://docs.coiled.io) for more complete information on Coiled. Including:\n",
    "    - [Launching clusters with GPUs](https://docs.coiled.io/user_guide/gpu.html)\n",
    "    - [End to end network security](https://docs.coiled.io/user_guide/security.html)\n",
    "    - ...\n",
    "- Join the [Coiled community Slack](https://join.slack.com/t/coiled-users/shared_invite/zt-hx1fnr7k-In~Q8ui3XkQfvQon0yN5WQ) to chat about Coiled and Dask, ask questions, and sharing tips with other Coiled users and the Coiled engineering team\n",
    "- If you run into a bug or have a feature request, please feel free to open an issue on the [Coiled issue tracker](https://github.com/coiled/coiled-issues)\n",
    "\n",
    "# Thank you all for your time and attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
